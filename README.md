# MANE-4962-Final-Project
Final Project for MANE 4962

# Project Title: MACHINE LEARNING FOR SPACE DOMAIN AWARENESS
by Calvin Chan

# Brief Description: 
Machine Learning concerning SDA
Utilizng machine learning algorithms to pefrorm classification and predictions
Both traditional methods, like EKF, and machine learning architectures like deep neural networks and long-short term memory. 

# Dataset Used:
The dataset was generated by notebook codes. 
The periodic orbit dataset was taken from the JPL database (CR3BP database)
Cite: https://ssd.jpl.nasa.gov/tools/periodic_orbits.html (BY: NASA)

# Model/Method Overview:
Two Models were tested:
Deep Neural Network (DNN)
  - 7 Layers
  - 512 cells per layer
  - ReLU activation Function
  - Adam Optimizer 0.001 Learning Rate
  - MSE Loss Function
Long Short-Term Memory (LSTM)
  - 3 Layers
  - 512 cells per layer
  - 3 Dropout Layers to prevent overfitting
  - Adam Optimizer 0.001 Learning Rate
  - MSE Loss Function

In addition, a DNN model was trained using EKF-generated Data to simulate a noisy dataset and sparse measurements. 

# Instruction to Run Code: 
Download all the codes from the notebook and the src
Notebook folder codes have only the classification and prediction codes. 
The SRC folder has all the related fucntion to run the codes.
Download the dataset folder, which contains numpy arrays and one dataset for classification. 

Code can be run once all available notebooks are downloaded. 
In general, DNNmodel = runs the DNN model code
LSTMmodel = runs the LSTM model code
EKFtrainDNN = runs the DNN train EKF model. 
Embedded in the EKFtrainDNN is a commented-out data generation that allows the user to specify how many datapoints can be generated.
One needs to adjust the solve_ivp settings parameter teval to the desired time index. 

Each notebook has comments describing the functions and lines
The hyperparameters of the model can be played with and tuned to desired liking.

# Summary of Results: 
R2 Scores: 
DNN = 0.996
LSTM = 0.734 
DNN trained EKF = 0.998

Plot results can be seen in the results folders. 

Results are sensitive to the number of datapoints and epochs. 
One can simulate these sensitivities by playing around with the number of data points generated and the number of epochs. 

